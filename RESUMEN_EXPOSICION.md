# ğŸŒ± PlantCare AI - Script de ExposiciÃ³n Completo
## Proyecto Final - IntroducciÃ³n a la Inteligencia Artificial

## â±ï¸ TIMING: 15 minutos (7.5 min cada uno)

---

## ğŸ‘¤ PERSONA 1: Fundamentos y Arquitectura (7.5 minutos)

### 1ï¸âƒ£ IntroducciÃ³n (1 min)

**Decir exactamente:**

"Buenos dÃ­as/tardes. Somos [Nombres] y hoy presentamos **PlantCare AI**, un sistema de inteligencia artificial diseÃ±ado para ayudar a las personas a cuidar sus plantas de forma inteligente.

Este proyecto integra los conceptos fundamentales de IA que aprendimos en el curso: **embeddings y similitud**, **bases de datos vectoriales**, **extracciÃ³n de datos con APIs**, y una **arquitectura multiagente construida con LangChain**.

El sistema permite a los usuarios subir una imagen de su planta y recibir un anÃ¡lisis completo que incluye identificaciÃ³n de la especie, diagnÃ³stico de salud, y recomendaciones personalizadas basadas en conocimiento especializado."

---

### 2ï¸âƒ£ Arquitectura Multi-Agente (2 min)

**Decir exactamente:**

"El corazÃ³n de nuestro sistema es una **arquitectura de 4 agentes especializados** que trabajan juntos para proporcionar anÃ¡lisis completos.

*[Mostrar diagrama de flujo]*

Primero, tenemos el **Agente de VisiÃ³n**. Este agente se encarga de analizar la imagen de la planta. Utiliza dos APIs: Plant.id para identificar la especie de la planta, y Gemini Vision para analizar el estado de salud visual, detectando problemas como manchas, hojas amarillas, o plagas.

Segundo, el **Agente de Conocimiento**. Este agente busca informaciÃ³n relevante en nuestra base de datos vectorial. Cuando el usuario pregunta algo como 'Â¿CÃ³mo cuido una suculenta?', este agente genera un embedding de la pregunta y busca documentos similares en Supabase usando bÃºsqueda vectorial.

Tercero, el **Agente de AnÃ¡lisis**. Este agente combina toda la informaciÃ³n: los datos visuales del Agente de VisiÃ³n, el conocimiento recuperado del Agente de Conocimiento, y las acciones que el usuario ha mencionado. Aplica reglas de diagnÃ³stico para identificar problemas especÃ­ficos como exceso de riego o falta de luz, y calcula una puntuaciÃ³n de salud del 1 al 10.

Y finalmente, el **Agente de Respuesta**, que actÃºa como orquestador. Este agente coordina a todos los demÃ¡s usando LangChain AgentExecutor, y utiliza un LLM - en nuestro caso Gemini u Ollama local - para generar recomendaciones finales personalizadas y en lenguaje natural.

Â¿Por quÃ© usar mÃºltiples agentes en lugar de uno solo? Porque cada agente tiene una responsabilidad especÃ­fica y clara. Esto hace que el sistema sea mÃ¡s modular, fÃ¡cil de mantener, y permite que cada agente se especialice en su tarea."

---

### 3ï¸âƒ£ Embeddings y Supabase (2 min)

**Decir exactamente:**

"Ahora voy a explicar cÃ³mo funciona la bÃºsqueda de informaciÃ³n en nuestro sistema, que es uno de los componentes mÃ¡s importantes.

*[Abrir `src/embeddings.py`]*

Primero, los **embeddings**. Un embedding es una representaciÃ³n numÃ©rica del texto que captura su significado semÃ¡ntico. En nuestro cÃ³digo, usamos el modelo `sentence-transformers/all-MiniLM-L6-v2`, que convierte cualquier texto en un vector de 384 nÃºmeros.

*[Mostrar cÃ³digo]*

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Â¿CÃ³mo cuido una suculenta?")
# Resultado: array de 384 nÃºmeros como [0.23, -0.45, 0.12, ...]
```

Estos 384 nÃºmeros no son aleatorios. Cada posiciÃ³n en el vector captura algÃºn aspecto del significado del texto. Textos similares tendrÃ¡n vectores similares, y textos diferentes tendrÃ¡n vectores diferentes.

*[Abrir `src/agentes/agente_conocimiento.py` o `src/vector_db.py`]*

Ahora, para buscar informaciÃ³n relevante, usamos **Supabase con pgvector**. Supabase es PostgreSQL en la nube, y pgvector es una extensiÃ³n que permite almacenar y buscar vectores eficientemente.

*[Mostrar cÃ³digo]*

```python
# Generar embedding de la consulta
query_embedding = self.embedding_generator.generate_embedding("Â¿CÃ³mo cuido una suculenta?")

# Buscar en Supabase usando pgvector
results = supabase.rpc('match_plant_documents', {
    'query_embedding': query_embedding.tolist(),  # Convertir a lista
    'match_threshold': 0.3,  # Umbral de similitud mÃ­nimo
    'match_count': 5  # Top 5 documentos
})
```

La funciÃ³n `match_plant_documents` en Supabase calcula la **similitud del coseno** entre el embedding de la consulta y todos los embeddings almacenados. La similitud del coseno mide quÃ© tan similares son dos vectores, retornando un valor entre -1 y 1, donde 1 significa idÃ©nticos y -1 significa opuestos.

Esto nos permite encontrar documentos relevantes incluso si el usuario no usa las palabras exactas que estÃ¡n en nuestros documentos. Por ejemplo, si el usuario pregunta 'riego de suculentas' y nuestro documento dice 'cuidado de suculentas con agua', el sistema los encontrarÃ¡ como similares porque tienen embeddings cercanos."

---

### 4ï¸âƒ£ Flujo Completo (2.5 min)

**Decir exactamente:**

"Ahora voy a explicar el flujo completo de cÃ³mo todos estos componentes trabajan juntos cuando un usuario sube una imagen.

*[Mostrar diagrama de flujo paso a paso]*

**Paso 1**: El usuario sube una imagen de su planta y opcionalmente escribe una pregunta o preocupaciÃ³n, como 'le arranquÃ© una hoja sin querer'.

**Paso 2**: El **Agente de VisiÃ³n** se ejecuta. Primero intenta identificar la especie usando Plant.id API. Si eso falla o la confianza es muy baja, usa Gemini Vision como respaldo. Luego analiza la salud visual de la planta, detectando problemas como hojas amarillas o manchas.

**Paso 3**: Con la especie identificada y los problemas visuales detectados, el **Agente de Conocimiento** construye una consulta mejorada. Por ejemplo, si la especie es 'Cestrum nocturnum' y hay problemas de 'hojas amarillas', la consulta serÃ­a algo como 'Cestrum nocturnum hojas amarillas cuidado'. Esta consulta se convierte en un embedding y se busca en Supabase, retornando los 5 documentos mÃ¡s relevantes.

**Paso 4**: El **Agente de AnÃ¡lisis** recibe toda esta informaciÃ³n: los datos visuales, el conocimiento recuperado, y las acciones del usuario. Aplica reglas de diagnÃ³stico. Por ejemplo, si el usuario dice 'riego cada dÃ­a' y hay hojas amarillas, identifica 'exceso de riego' con una severidad alta. Calcula una puntuaciÃ³n de salud final considerando el estado visual y los problemas identificados.

**Paso 5**: Finalmente, el **Agente de Respuesta** toma todo este anÃ¡lisis y lo pasa a un LLM - Gemini u Ollama local - junto con el conocimiento relevante. El LLM genera recomendaciones personalizadas y en lenguaje natural. Si el usuario mencionÃ³ una preocupaciÃ³n especÃ­fica, como 'arranquÃ© una hoja', el LLM aborda directamente esa preocupaciÃ³n, tranquilizando al usuario y dando consejos especÃ­ficos.

Todo este flujo estÃ¡ orquestado por LangChain AgentExecutor, que coordina la ejecuciÃ³n de cada agente de forma inteligente."

---

## ğŸ‘¤ PERSONA 2: ImplementaciÃ³n y Demo (7.5 minutos)

### 5ï¸âƒ£ LangChain y OrquestaciÃ³n (2 min)

**Decir exactamente:**

"Ahora voy a mostrar cÃ³mo implementamos la orquestaciÃ³n de agentes usando LangChain.

*[Abrir `src/agentes/agente_respuesta_langchain.py`]*

LangChain es un framework diseÃ±ado especÃ­ficamente para construir aplicaciones con LLMs y agentes. En nuestro caso, usamos **AgentExecutor**, que es el componente que coordina la ejecuciÃ³n de mÃºltiples agentes.

*[Mostrar cÃ³digo de creaciÃ³n de Tools, lÃ­neas 137-200 aproximadamente]*

Primero, convertimos cada agente en una **Tool** - una herramienta que el LLM puede usar. Por ejemplo:

```python
def vision_tool(image_path: str, user_context: str) -> str:
    """Herramienta para anÃ¡lisis visual"""
    result = self.vision_agent.execute(image_path, user_context)
    return f"Especie: {result['species']}, Salud: {result['health_score']}/10"

tools = [
    Tool(name="vision_analysis", func=vision_tool, 
         description="Analiza imÃ¡genes de plantas"),
    Tool(name="knowledge_search", func=knowledge_tool,
         description="Busca informaciÃ³n en base de conocimiento"),
    Tool(name="plant_analysis", func=analysis_tool,
         description="Realiza anÃ¡lisis y diagnÃ³stico")
]
```

*[Mostrar cÃ³digo de AgentExecutor, lÃ­neas 207-250 aproximadamente]*

Luego, creamos el **AgentExecutor**:

```python
from langchain.agents import AgentExecutor, create_structured_chat_agent

agent = create_structured_chat_agent(
    llm=self.llm,  # Gemini u Ollama
    tools=self.tools,  # Las 3 herramientas
    prompt=prompt  # Instrucciones para el agente
)

executor = AgentExecutor(
    agent=agent,
    tools=self.tools,
    memory=self.memory,  # Memoria de conversaciÃ³n
    verbose=True
)
```

El AgentExecutor es inteligente: el LLM decide quÃ© herramienta usar y en quÃ© orden, basÃ¡ndose en el contexto. Por ejemplo, si el usuario pregunta sobre una planta, el LLM primero usarÃ¡ `vision_analysis`, luego `knowledge_search`, luego `plant_analysis`, y finalmente generarÃ¡ la respuesta.

Esto es diferente a una implementaciÃ³n manual donde nosotros controlamos el flujo explÃ­citamente. Con LangChain, el LLM puede tomar decisiones inteligentes sobre cÃ³mo orquestar los agentes."

---

### 6ï¸âƒ£ LLM y GeneraciÃ³n de Respuestas (2 min)

**Decir exactamente:**

"Ahora voy a explicar cÃ³mo usamos los LLMs - Large Language Models - para generar respuestas.

*[Abrir `src/agentes/agente_respuesta.py`, secciÃ³n de generate_recommendations]*

Un **LLM** es un modelo de lenguaje que puede generar texto basado en contexto. En nuestro sistema, usamos principalmente **Google Gemini**, pero tambiÃ©n tenemos un fallback a **Ollama** que corre localmente.

*[Mostrar cÃ³digo]*

```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Configurar Gemini
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7  # Controla la creatividad
)

# Generar recomendaciones
prompt = f"""Eres un experto en plantas. BasÃ¡ndote en:
- Especie: {species}
- DiagnÃ³stico: {diagnosis}
- Conocimiento: {context}
- Pregunta del usuario: {user_question}

Genera 3-5 recomendaciones especÃ­ficas."""
```

El LLM recibe todo este contexto y genera recomendaciones en lenguaje natural. Si el usuario tiene una preocupaciÃ³n especÃ­fica, como 'arranquÃ© una hoja', el LLM la aborda directamente.

*[Mostrar cÃ³digo de fallback]*

```python
# Si Gemini falla, usar Ollama local
if not self.llm:
    from src.local_llm import get_local_llm
    local_llm = get_local_llm()  # Ollama con llama3.2
    response = local_llm.generate(prompt)
```

Â¿Por quÃ© tener mÃºltiples fallbacks? Porque las APIs externas pueden fallar o agotar su cuota. Nuestro sistema tiene esta cadena: Gemini â†’ Ollama local â†’ Solo documentos â†’ Modo demo. Esto asegura que el sistema siempre funcione, incluso sin conexiÃ³n a internet o sin APIs externas."

---

### 7ï¸âƒ£ Ejemplos de CÃ³digo Clave (3 min)

**Decir exactamente:**

"Ahora voy a mostrar ejemplos especÃ­ficos de cÃ³digo de cada agente para que vean cÃ³mo estÃ¡ implementado.

*[Abrir `src/agentes/agente_vision.py`, funciÃ³n identify_plant_species]*

**Primer ejemplo: Agente de VisiÃ³n**

AquÃ­ vemos cÃ³mo el agente identifica la especie de la planta. Primero intenta con Plant.id API:

```python
def identify_plant_species(self, image_path: str):
    # Codificar imagen en base64
    with open(image_path, 'rb') as f:
        image_data = base64.b64encode(f.read()).decode('utf-8')
    
    # Llamar a Plant.id API
    response = requests.post(
        "https://api.plant.id/v2/identify",
        json={"images": [f"data:image/jpeg;base64,{image_data}"]},
        headers={"Api-Key": self.plant_id_key}
    )
    
    species = response.json()['suggestions'][0]['plant_name']
    probability = response.json()['suggestions'][0]['probability']
    
    return {'species': species, 'probability': probability}
```

Si esto falla, usa Gemini Vision como respaldo. Esto es importante porque garantiza que siempre tengamos una identificaciÃ³n, aunque sea con menor confianza.

*[Abrir `src/agentes/agente_conocimiento.py`, funciÃ³n search_knowledge]*

**Segundo ejemplo: Agente de Conocimiento**

Este es el cÃ³digo que realiza la bÃºsqueda vectorial:

```python
def search_knowledge(self, query: str, species: str, problems: List[str]):
    # 1. Construir consulta mejorada
    enhanced_query = f"{species} {query} problemas: {', '.join(problems)}"
    
    # 2. Generar embedding
    query_embedding = self.embedding_generator.generate_embedding(enhanced_query)
    
    # 3. Buscar en Supabase con pgvector
    results = self.vector_db.search_similar(
        query_embedding, 
        top_k=5, 
        threshold=0.25
    )
    
    return documents  # Ordenados por relevancia
```

La funciÃ³n `search_similar` internamente llama a la funciÃ³n SQL `match_plant_documents` en Supabase, que usa el operador `<=>` de pgvector para calcular la distancia entre vectores. Esto es muy eficiente incluso con miles de documentos.

*[Abrir `src/agentes/agente_respuesta_langchain.py`, secciÃ³n de _create_agent_executor]*

**Tercer ejemplo: LangChain AgentExecutor**

Este es el cÃ³digo que crea el orquestador:

```python
def _create_agent_executor(self):
    # Crear Tools
    tools = [
        Tool(name="vision_analysis", func=vision_tool, ...),
        Tool(name="knowledge_search", func=knowledge_tool, ...),
        Tool(name="plant_analysis", func=analysis_tool, ...)
    ]
    
    # Crear agente estructurado
    agent = create_structured_chat_agent(
        llm=self.llm,
        tools=tools,
        prompt=prompt
    )
    
    # Crear ejecutor
    executor = AgentExecutor(
        agent=agent,
        tools=tools,
        memory=self.memory
    )
    
    return executor
```

El AgentExecutor usa el LLM para decidir quÃ© tool usar. El LLM lee las descripciones de las tools y decide automÃ¡ticamente el orden de ejecuciÃ³n basÃ¡ndose en el contexto de la conversaciÃ³n."

---

### 8ï¸âƒ£ DemostraciÃ³n PrÃ¡ctica (1 min)

**Decir exactamente:**

"Ahora voy a mostrar cÃ³mo funciona el sistema en tiempo real.

*[Abrir terminal con backend corriendo, mostrar logs]*

Cuando procesamos una imagen, podemos ver en los logs cÃ³mo cada agente se ejecuta secuencialmente:

```
ğŸ” AGENTE DE VISIÃ“N ejecutando...
  âœ“ Plant.id identificÃ³: Cestrum nocturnum (confianza: 32%)
  âœ“ Estado de salud: Regular (5/10)

ğŸ” AGENTE DE CONOCIMIENTO ejecutando...
  Consulta: Cestrum nocturnum Â¿CÃ³mo la cuido?
  âœ“ Encontrados 5 documentos relevantes

ğŸ”¬ AGENTE DE ANÃLISIS ejecutando...
  âœ“ DiagnÃ³stico: Estado general saludable
  âœ“ Problemas identificados: falta_de_luz

ğŸ’¡ Generando recomendaciones...
  âœ“ LangChain LLM generÃ³ 4 recomendaciones
```

Cada agente pasa su resultado al siguiente, y al final tenemos un anÃ¡lisis completo que incluye la especie identificada, el estado de salud, el diagnÃ³stico, y recomendaciones personalizadas.

El sistema estÃ¡ diseÃ±ado para ser robusto: si una API falla, hay un fallback. Si el LLM no estÃ¡ disponible, usa solo los documentos. Esto asegura que siempre podamos dar una respuesta Ãºtil al usuario."

---

### 9ï¸âƒ£ Conclusiones (0.5 min)

**Decir exactamente:**

"Para concluir, nuestro proyecto **PlantCare AI** demuestra la integraciÃ³n exitosa de todos los componentes requeridos:

âœ… **Fuente de datos**: Documentos de plantas almacenados en Supabase
âœ… **ExtracciÃ³n**: APIs como Plant.id y Gemini Vision para analizar imÃ¡genes
âœ… **SegmentaciÃ³n**: Texto dividido en chunks para bÃºsqueda eficiente
âœ… **Embeddings**: Usando sentence-transformers para convertir texto en vectores
âœ… **Similitud**: BÃºsqueda vectorial con similitud del coseno usando pgvector
âœ… **Base de datos vectorial**: Supabase con extensiÃ³n pgvector
âœ… **Arquitectura multiagente**: 4 agentes coordinados con LangChain AgentExecutor
âœ… **Interfaz**: Frontend web interactivo

El sistema muestra cÃ³mo estos conceptos fundamentales de IA se combinan para crear una aplicaciÃ³n prÃ¡ctica y funcional. Gracias por su atenciÃ³n. Â¿Hay alguna pregunta?"

---

## ğŸ¯ PUNTOS CLAVE A RECORDAR

### âœ… Cumplimos todos los requisitos:
1. âœ… Fuente de datos (documentos en Supabase)
2. âœ… ExtracciÃ³n (APIs: Plant.id, Gemini Vision)
3. âœ… SegmentaciÃ³n (chunks de texto)
4. âœ… Embeddings (sentence-transformers)
5. âœ… Similitud (coseno con pgvector)
6. âœ… Base vectorial (Supabase + pgvector)
7. âœ… Arquitectura multiagente (LangChain)
8. âœ… Interfaz (frontend web)

### ğŸ”‘ Conceptos tÃ©cnicos:
- **Embedding**: Texto â†’ Vector numÃ©rico (384 dims)
- **Similitud del coseno**: Compara vectores
- **pgvector**: BÃºsqueda vectorial en PostgreSQL
- **LangChain**: Framework para orquestar agentes
- **AgentExecutor**: Coordina ejecuciÃ³n de agentes
- **RAG**: Retrieval-Augmented Generation

---

## ğŸ“Š DIAGRAMA SIMPLIFICADO

```
Usuario â†’ [Imagen + Pregunta]
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ ResponseAgentâ”‚ â† LangChain AgentExecutor
    â”‚ (Orquestador)â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚      â”‚      â”‚
    â–¼      â–¼      â–¼      â–¼
[Vision] [Know] [Anal] [LLM]
    â”‚      â”‚      â”‚      â”‚
    â–¼      â–¼      â–¼      â–¼
[APIs] [Supabase] [LÃ³gica] [Gemini]
         â”‚
         â–¼
    [pgvector]
    [Embeddings]
```

---

## ğŸ’¡ FRASES CLAVE PARA USAR

- "Usamos embeddings para convertir texto en nÃºmeros que capturan su significado"
- "Supabase con pgvector nos permite buscar documentos similares eficientemente"
- "LangChain AgentExecutor coordina los agentes de forma inteligente"
- "El sistema tiene mÃºltiples fallbacks para funcionar sin APIs externas"
- "Cada agente tiene una responsabilidad especÃ­fica y clara"

---

## ğŸš€ DEMOSTRACIÃ“N RÃPIDA

**Si tienen tiempo, mostrar:**
1. Abrir terminal con backend corriendo
2. Mostrar logs cuando se procesa una imagen
3. Explicar cada paso que aparece en los logs
4. Mostrar resultado final formateado

---

**Â¡Ã‰xito! ğŸŒ¿**

